# -------------------------------------------------------------
# 1. IMPORT LIBRARIES
# -------------------------------------------------------------
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# -------------------------------------------------------------
# 2. LOAD INBUILT DATASET (20 NEWGROUPS)
# -------------------------------------------------------------
categories = ['sci.space', 'rec.sport.hockey', 'comp.graphics']
train = fetch_20newsgroups(subset='train', categories=categories)
test = fetch_20newsgroups(subset='test', categories=categories)

print("Sample Data:", train.data[0][:500])
print("Total Training Samples:", len(train.data))

# -------------------------------------------------------------
# 3. TEXT PREPROCESSING FUNCTIONS
# -------------------------------------------------------------
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess_stemming(text):
    tokens = nltk.word_tokenize(text.lower())
    tokens = [stemmer.stem(w) for w in tokens if w.isalpha() and w not in stop_words]
    return " ".join(tokens)

def preprocess_lemmatization(text):
    tokens = nltk.word_tokenize(text.lower())
    tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words]
    return " ".join(tokens)

# -------------------------------------------------------------
# 4. APPLY STEMMING AND LEMMATIZATION ON DATASET
# -------------------------------------------------------------
train_stemmed = [preprocess_stemming(doc) for doc in train.data]
test_stemmed = [preprocess_stemming(doc) for doc in test.data]

train_lemma = [preprocess_lemmatization(doc) for doc in train.data]
test_lemma = [preprocess_lemmatization(doc) for doc in test.data]

# -------------------------------------------------------------
# 5. CREATE TF-IDF + NAIVE BAYES PIPELINES
# -------------------------------------------------------------
model_stem = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

model_lemma = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', MultinomialNB())
])

# -------------------------------------------------------------
# 6. TRAIN MODELS
# -------------------------------------------------------------
model_stem.fit(train_stemmed, train.target)
model_lemma.fit(train_lemma, train.target)

# -------------------------------------------------------------
# 7. PREDICT & COMPARE ACCURACY
# -------------------------------------------------------------
pred_stem = model_stem.predict(test_stemmed)
pred_lemma = model_lemma.predict(test_lemma)

accuracy_stem = accuracy_score(test.target, pred_stem)
accuracy_lemma = accuracy_score(test.target, pred_lemma)

print("\n--------------------------")
print("ACCURACY COMPARISON")
print("--------------------------")
print("Accuracy using STEMMING:     ", accuracy_stem)
print("Accuracy using LEMMATIZATION:", accuracy_lemma)
